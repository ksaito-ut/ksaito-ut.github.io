<!DOCTYPE html>
<html>

<head>
<meta charset="utf-8">
<title>[ Kuniaki Saito's Webpage ]</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- Font Awesome Icons -->
<link rel="stylesheet" href="css/font-awesome.min.css">
<!-- Bootstrap -->
<link href="css/bootstrap.min.css" rel="stylesheet">

<link href="css/custom.css" rel="stylesheet">
<link rel="stylesheet" href="css/main.css">

<!--<link href="css/bootstrap.min.css" rel="stylesheet">-->

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script defer src="js/html5shiv.js"></script>
<script defer src="js/respond.min.js"></script>
<![endif]-->

<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
<script defer src="js/jquery.min.js"></script>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script defer src="js/bootstrap.min.js"></script>
<script defer src="js/menucollapse.js"></script>
<script defer type="text/javascript" src="js/arrow78.js"></script>
<script defer type="text/javascript" src="js/custom.js"></script>

<body id="page-top" class="index">
  <!--<nav class="navbar navbar-default navbar-fixed-top">-->

  <<nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container-fluid">
          <!-- Brand and toggle get grouped for better mobile display -->
          <div class="navbar-header">
              <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                  <span class="sr-only">Toggle navigation</span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
              </button>
              <!-- <span><a href="#"><img border="0" height="50" src="images/sapienza.png"/></a></span>
          </div>-->
          <a class="navbar-brand" href="index.html">Kuniaki Saito's Webpage</a>
          </div>
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
              <ul class="nav navbar-nav navbar-left">
                  <li class="dropdown">
                    <li class="page-scroll">
                        <a href="index.html">Home</a>
                    </li>
                  <li class="dropdown">
                  <!-- <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                  Publications<span class="caret"></span></a>
                      <ul class="dropdown-menu">
                      <li><a onclick="javascript:reset_menus();$('#tab-3-content').show();" href="index.html#publications">Selected</a></li>
                          <li><a href="publications.html">All</a></li>
                      </ul>
                  </li>-->
                  <li class="page-scroll">
                      <a href="projects.html">Research Project</a>
                  </li>
                  <!-- <li class="page-scroll">
                  <a href="index.html#advising">Advising</a>
                  </li> -->
                  <!-- <li class="page-scroll">
                      <a target="_blank" href="">Grants</a>
                  </li> -->

              </ul>
          </div><!-- /.navbar-collapse -->

      </div><!-- /.container-fluid -->
  </nav>


    <!-- Page Content -->
	<div class="container">

		<!-- Page heading -->
    padding-top: 10px;
        <div class="row">
            <div class="col-lg-12">
                <h2 class="page-header">Research Projects</h2>

			<!-- paraphraph of text
            <div class="col-md-9">
                <h2>Section Heading</h2>
                <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit. Soluta, et temporibus, facere perferendis veniam beatae non debitis, numquam blanditiis necessitatibus vel mollitia dolorum laudantium, voluptate dolores iure maxime ducimus fugit.</p>
            </div>
			-->
                <!-- Project -->

  <div class="row">
    <div class="col-md-5">
      <a href="">
<center><img class="img-responsive img-hover" src="images/teaser_github.png"  width = "400 px" ></center>          </a>
    </div>
    <div class="col-md-7">
      <h4> Mind the Backbone: Minimizing Backbone Distortion for Robust Object Detection 
      </h4>
      <p><em> <b>Kuniaki Saito,</b>  Donghyun Kim, Piotr Teterwak, Rogerio Feris, and Kate Saenko <b>Arxiv 2023</b></em></p>
    <p>
    </p>
      <a class="btn btn-primary" href="">Project</i></a>
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2303.14744.pdf">Paper</i></a>
        <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/mind_back">Code</i></a>

    </div>
</div>
<hr>
              
              

  <div class="row">
    <div class="col-md-5">
      <a href="">
<center><img class="img-responsive img-hover" src="images/pic2word.png"  width = "300 px" ></center>          </a>
    </div>
    <div class="col-md-7">
      <h4> Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval
      </h4>
      <p><em> <b>Kuniaki Saito,</b> Kihyuk Sohn,Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, Tomas Pfister <b>CVPR 2023</b></em></p>
    <p>
    </p>
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2302.03084.pdf">Paper</i></a>
    </div>
</div>
<hr>



<div class="row">
    <div class="col-md-5">
      <a href="">
          <img class="img-responsive img-hover" src="images/vision_language/figure_main.png"  width = "450 px" ></a>
    </div>
    <div class="col-md-7">
      <h4> Prefix Conditioning Unifies Language and Label Supervision
      </h4>
      <p><em> <b>Kuniaki Saito,</b> Kihyuk Sohn,Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, Tomas Pfister <b>CVPR 2023</b></em></p>
    <p>
    </p>
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2206.01125.pdf">Paper</i></a>
    </div>
</div>
<hr>


<div class="row">
    <div class="col-md-5">
      <a href="">
          <img class="img-responsive img-hover" src="images/ldet/ldet_teaser.gif"  width = "450 px" ></a>
    </div>
    <div class="col-md-7">
      <h4> Learning to Detect Every Thing in an Open World
      </h4>
      <p><em> <b>Kuniaki Saito,</b> Ping Hu, Trevor Darrell, and Kate Saenko   <b>ECCV 2022</b></em></p>
    <p>
    </p>
    <!-- <a class="btn btn-primary" href="https://arxiv.org/pdf/2105.14148.pdf">Paper</i></a>-->
        <a class="btn btn-primary" href="https://ksaito-ut.github.io/openworld_ldet/">Project</i></a>
        <a class="btn btn-primary" href="https://github.com/ksaito-ut/openworld_ldet">Code</i></a>
    </div>
</div>
<hr>


<div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/pdf/2105.14148.pdf">
          <img class="img-responsive img-hover" src="images/OpenMatch/teaser.webp"  width = "450 px" ></a>
    </div>
    <div class="col-md-7">
      <h4> OpenMatch:Open-set Consistency Regularization for Semi-supervised Learning with Outliers
      </h4>
      <p><em> <b>Kuniaki Saito,</b> Donghyun Kim,  and Kate Saenko   <b>Neurips 2021</b></em></p>
    <p>
    </p>
    <a class="btn btn-primary" href="https://arxiv.org/pdf/2105.14148.pdf">Paper</i></a>
        <a class="btn btn-primary" href="images/OpenMatch/openmatch.pdf">Slides</i></a>
        <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/OP_Match">Code</i></a>
    </div>
</div>
<hr>


<div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/pdf/2104.03344v3.pdf">
        <img class="img-responsive img-hover" src="images/SND/snd_fig.webp"  width = "350 px" ></a>
    </div>
    <div class="col-md-7">
      <h4> Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density
      </h4>
      <p><em> <b> Kuniaki Saito </b>, Donghyun Kim, Piotr Teterwak, Stan Sclaroff, Trevor Darrell, and Kate Saenko  <b>ICCV 2021</b></em></p>
    <p>
    </p>
    <a class="btn btn-primary" href="https://arxiv.org/pdf/2108.10860.pdf">Paper</i></a>
        <a class="btn btn-primary" href="./research/SND.html">View Project</i></a>
        <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/SND">Code</i></a>

    </div>
    </div>
    <hr>


<div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/pdf/2104.03344v3.pdf">
        <video width="450 px" autoplay loop muted playsinline>
           <source src="images/ovanet/animation_ovanet_project.webm" type="video/webm">
           <source src="images/ovanet/animation_ovanet_project.mp4" type="video/mp4">
           </video>
      </a>
    </div>
    <div class="col-md-7">
      <h4> OVANet: One-vs-All Network for Universal Domain Adaptation
      </h4>
      <p><em> <b>Kuniaki Saito</b> and Kate Saenko   <b>ICCV 2021</b></em></p>
    <p>
    </p>
    <a class="btn btn-primary" href="https://arxiv.org/pdf/2104.03344v3.pdf">Paper</i></a>
        <a class="btn btn-primary" href="images/ovanet/OVANet_presentation.pdf">Slides</i></a>
    <a class="btn btn-primary" href="./research/OVANet.html">View Project</i></a>
      <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/OVANet">Github</i></a>
    </div>
    </div>
    <hr>


<div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/pdf/1904.06487.pdf">
        <img class="img-responsive img-hover" src="images/dance/fig1.webp"  width = "450 px" ></a>
    </div>
    <div class="col-md-7">
      <h4> Universal Domain Adaptation through Self-Supervision
      </h4>
      <p><em> <b>Kuniaki Saito</b>, Donghyun Kim, Stan Sclaroff, and Kate Saenko   <b>NeurIPS2020</b></em></p>
    <p>
    </p>
    <a class="btn btn-primary" href="https://arxiv.org/pdf/2002.07953.pdf">Paper</i></a>
    <a class="btn btn-primary" href="./research/DANCE.html">View Project</i></a>
      <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/DANCE">Github</i></a>
    </div>
    </div>
    <hr>

     <div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/abs/2007.07431">
        <video width="450 px" autoplay loop muted playsinline>
           <source src="images/cocofunit/teaser_new.webm" type="video/webm">
           <source src="images/cocofunit/teaser_new.mp4" type="video/mp4">
           </video>
    </a>
    </div>
    <div class="col-md-7">
      <h4> COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder
      </h4>
      <p><em> <b>Kuniaki Saito</b>, Kate Saenko and Ming-Yu Liu   <b>ECCV2020</b></em></p>
    <p>
        Unsupervised image-to-image translation intends to learn a mapping of an image in a given domain to an analogous image in a different domain, without explicit supervision of the mapping. Few-shot unsupervised image-to-image translation further attempts to generalize the model to an unseen domain by leveraging example images of the unseen domain provided at inference time. While remarkably successful, existing few-shot image-to-image translation models find it difficult to preserve the structure of the input image while emulating the appearance of the unseen domain, which we refer to as the content loss problem. This is particularly severe when the poses of the objects in the input and example images are very different. To address the issue, we propose a new few-shot image translation model, which computes the style embedding of the example images conditioned on the input image and a new architecture design called the universal style bias. Through extensive experimental validations with comparison to the state-of-the-art, our model shows effectiveness in addressing the content loss problem.
    </p>
    <a class="btn btn-primary" href="https://arxiv.org/abs/2007.07431">Paper</i></a>
    <a class="btn btn-primary" href="https://nvlabs.github.io/COCO-FUNIT/">View Project</i></a>
    </div>
    </div>
    <hr>


    <div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/pdf/1904.06487.pdf">
        <img class="img-responsive img-hover" src="images/mme/fig1.webp"  width = "450 px" ></a>
    </div>
    <div class="col-md-7">
      <h4> Semi-supervised Domain Adaptation via Minimax Entropy
      </h4>
      <p><em> <b>Kuniaki Saito</b>, Donghyun Kim, Stan Sclaroff, Trevor Darrell and Kate Saenko   <b>ICCV2019</b></em></p>
    <p> We proposed a method for Semi-supervised Domain Adaptation where a few labeled target examples are available in addition to unlabeled target examples.
        We proposed a new adversarial training method to solve the problem.
    </p>
    <a class="btn btn-primary" href="https://arxiv.org/pdf/1904.06487.pdf">Paper</i></a>
    <a class="btn btn-primary" href="./research/MME2019.html">View Project</i></a>
      <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/SSDA_MME">Github</i></a>
    </div>
    </div>
    <hr>

      <!-- Project -->
    <div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/abs/1812.04798">
        <img class="img-responsive img-hover" src="images/swda/swda.webp"  width = "400 px"></a>
    </div>
    <div class="col-md-7">
      <h4> Strong-Weak Distribution Alignment for Adaptive Object Detection
      </h4>
      <p><em> <b>Kuniaki Saito</b>, Yoshitaka Ushiku, Tatsuya Harada and Kate Saenko   <b>CVPR 2019</b></em></p>

    <p> We proposed a novel method for domain adaptive object detection. We performed weak-distribution alignment for high-level global feature whereas we also performed strong distribution aignment for low-level local feature. Our method outperformed other baselines with a large margin in four adaptation scenarios.</p>

    <a class="btn btn-primary" href="https://arxiv.org/abs/1812.04798">Paper</i></a>
    <a class="btn btn-primary" href="./research/CVPR2019.html">View Project</i></a>
        <a class="btn btn-primary" href="https://github.com/VisionLearningGroup/DA_Detection">Github</i></a>
    </div>
    </div>
    <hr>



      <!-- Project -->
  <div class="row">
    <div class="col-md-5">
      <a href="https://arxiv.org/abs/1804.10427v1">
        <img class="img-responsive img-hover" src="images/ODA_BP.webp"  width = "400 px"></a>
    </div>
    <div class="col-md-7">
      <h4> Open Set Domain Adaptation by Backpropagation
      </h4>
      <p><em> <b>Kuniaki Saito</b>, Shohei Yamamoto, Yoshitaka Ushiku and Tatsuya Harada,  <b>ECCV 2018</b></em></p>

    <p> We proposed a novel method for open set domain adaptation, where a target domain includes the category absent in a source domain. We propose to give a rejecting option to a classifier and achieve it by adversarial learning. Our method outperforms other methods in various tasks with a large margin.</p>


<a class="btn btn-primary" href="https://arxiv.org/abs/1804.10427v1">Paper</i></a>

  <a class="btn btn-primary" href="https://github.com/ksaito-ut/OPDA_BP">Github</i></a>
    </div>
  </div>
  <hr>

  <!-- Project -->
<div class="row">
<div class="col-md-5">
  <a href="https://arxiv.org/abs/1712.02560">
    <img class="img-responsive img-hover" src="images/MCD.webp"  width = "400 px"></a>
</div>
<div class="col-md-7">
  <h4> Maximum Classifier Discrepancy for Unsupervised Domain Adaptation
  </h4>
  <p><em> <b>Kuniaki Saito</b>, Kohei Watanabe, Yoshitaka Ushiku and Tatsuya Harada, <b>CVPR 2018</b> <font color="red">oral</font><br></em></p>

<p> We propose a new approach that attempts to align distributions of source and target domain by utilizing the task-specific decision boundaries. We propose to utilize task-specific classifiers as discriminators that try to detect target samples that are far from the support of the source. Our method outperforms other methods on several datasets of image classification and semantic segmentation. </p>


<a class="btn btn-primary" href="https://arxiv.org/abs/1712.02560">Paper</i></a>
<a class="btn btn-primary" href="https://mil-tokyo.github.io/MCD_DA/">View Project</i></a>
  <a class="btn btn-primary" href="https://github.com/mil-tokyo/MCD_DA">Code</i></a>
  <a class="btn btn-primary" href="images/cvpr_oral_slides.pdf">Slides</i></a>
</div>
</div>
<hr>

    <!-- Project -->
  <div class="row">
    <div class="col-md-5">
      <a href="https://openreview.net/pdf?id=HJIoJWZCZ">
        <img class="img-responsive img-hover" src="images/adr_da.webp"  width = "400 px"></a>
    </div>
    <div class="col-md-7">
      <h4> Adversarial Dropout Reguralization
      </h4>
      <p><em> <b>Kuniaki Saito</b>, Yoshitaka Ushiku, Tatsuya Harada and Kate Saenko,  <b>ICLR 2018</b></em></p>

    <p>  We proposed a novel method for unsupervised domain adaptation. The method is based on adversarial learning and effectively utilizes dropout. The method outperforms other methods on digits classification, object classification, and semantic segmentation tasks.</p>


<a class="btn btn-primary" href="https://openreview.net/pdf?id=HJIoJWZCZ">Paper</i></a>
      <a class="btn btn-primary" href="http://ai.bu.edu/adr_da/index.html">View Project</i></a>
  <a class="btn btn-primary" href="https://github.com/mil-tokyo/adr_da">Code</i></a>
    </div>
  </div>

    <hr>
  <!-- Project -->
  <div class="row">
  <div class="col-md-5">
    <a href="https://arxiv.org/pdf/1702.08400.pdf">
      <img class="img-responsive img-hover" src="images/ICML_2017.webp"  width = "400 px"></a>
  </div>
  <div class="col-md-7">
    <h4> Assymetric Tri-training for Unsupervised Domain Adaptation
    </h4>
    <p><em> <b>Kuniaki Saito</b>, Yoshitaka Ushiku and Tatsuya Harada  <b>ICML 2017</b></em></p>
  <p> We propose the use of an asymmetric tritraining method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train the neural networks as if they are true labels. In our work, we use three networks asymmetrically, and by asymmetric, we mean that two networks are used to label unlabeled target samples, and one network is trained by the pseudo-labeled samples to obtain target-discriminative representations. Our proposed method was shown to achieve a stateof-the-art performance on the benchmark digit recognition datasets for domain adaptation.</p>

  <a class="btn btn-primary" href="https://arxiv.org/pdf/1702.08400.pdf">Paper</i></a>

  </div>
  </div>
  <hr>
			<!-- Project
			<div class="row">
				<div class="col-md-5">
					<a href="">
						<img class="img-responsive img-hover" src="figs/"  width = "500 px"></a>
				</div>
				<div class="col-md-7">
					<h4> </h4>
					<p>  </p>
					<a class="btn btn-primary" href="">Paper</i></a>
				</div>
			</div>
			<hr>-->

        </div>
        <!-- /.row -->


        <!-- Pagination
        <div class="row text-center">
            <div class="col-lg-12">
                <ul class="pagination">
                    <li>
                        <a href="#">&laquo;</a>
                    </li>
                    <li class="active">
                        <a href="#">1</a>
                    </li>
                    <li>
                        <a href="#">2</a>
                    </li>
                    <li>
                        <a href="#">3</a>
                    </li>
                    <li>
                        <a href="#">4</a>
                    </li>
                    <li>
                        <a href="#">5</a>
                    </li>
                    <li>
                        <a href="#">&raquo;</a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- /.row -->

        <hr>





        <hr class="star-primary">
        <footer>
            <small>
            <center>
                © 2018 | Kuniaki Saito. Credits: AR template
            <a onclick="javascript:$('#credit').toggle();"><img border="0" src="images/ccby.png"/></a>
            <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
            <a href="https://github.com/dmsl/academic-responsive-template"_blank">
                https://github.com/dmsl/academic-responsive-template
            </a> ]
            </div>
            </center>
            </small>
        </footer>

        </body>

        </html>
